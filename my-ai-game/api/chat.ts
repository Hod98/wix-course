import type { VercelRequest, VercelResponse } from '@vercel/node';
import OpenAI from 'openai';

// CORS headers for development
const corsHeaders = {
  'Access-Control-Allow-Credentials': 'true',
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Methods': 'GET,OPTIONS,PATCH,DELETE,POST,PUT',
  'Access-Control-Allow-Headers': 'X-CSRF-Token, X-Requested-With, Accept, Accept-Version, Content-Length, Content-MD5, Content-Type, Date, X-Api-Version',
};

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  scenarioContext?: string;
}

export default async function handler(req: VercelRequest, res: VercelResponse) {
  // Handle CORS preflight
  if (req.method === 'OPTIONS') {
    return res.status(200).json({ body: 'OK' });
  }

  // Only allow POST requests
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { messages, scenarioContext }: ChatRequest = req.body;

    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Invalid request: messages required' });
    }

    // Get API key from environment
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      console.error('OPENAI_API_KEY not found in environment');
      return res.status(500).json({ error: 'Server configuration error' });
    }

    // Initialize OpenAI client (server-side, secure)
    const client = new OpenAI({
      apiKey,
      baseURL: process.env.OPENAI_BASE_URL || 'https://www.wixapis.com/openai/v1',
    });

    // Build the full message array with system prompt if scenario context is provided
    const fullMessages: ChatMessage[] = scenarioContext
      ? [
          {
            role: 'system',
            content: `××ª×” ×× ×”×œ ××©×—×§ ×”×¨×¤×ª×§××•×ª ×˜×§×¡×˜. ${scenarioContext}

×ª×¤×§×™×“×š:
â€¢ ×›×ª×•×‘ ×ª××™×“ ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“
â€¢ ×ª×Ÿ ×ª×©×•×‘×•×ª ×™×¦×™×¨×ª×™×•×ª ×•××¢× ×™×™× ×•×ª ×©×××©×™×›×•×ª ××ª ×”×¢×œ×™×œ×”
â€¢ ×©××•×¨ ×¢×œ ××•×•×™×¨×ª ×”××©×—×§ ×•×”×ª×¨×—×™×© ×”×¡×¤×¦×™×¤×™
â€¢ ×”×™×” ×ª××¦×™×ª×™ - 2-4 ××©×¤×˜×™× ×ª×™××•×¨×™×™×

× ×™×”×•×œ ×¤×¨×™×˜×™×:
×›××©×¨ ×”×©×—×§×Ÿ ××•×¦×, ××§×‘×œ ××• ××•×¡×£ ×¤×¨×™×˜, ×¦×™×™×Ÿ ×–××ª ×‘×¤×•×¨××˜ ×–×”:
[×§×™×‘×œ×ª: emoji ×©×_×¤×¨×™×˜]

×“×•×’×××•×ª ×œ×¤×¨×™×˜×™×:
[×§×™×‘×œ×ª: ğŸ—ï¸ ××¤×ª×—_×–×”×‘]
[×§×™×‘×œ×ª: ğŸ ×œ×—×_×˜×¨×™]
[×§×™×‘×œ×ª: âš”ï¸ ×—×¨×‘_×§×¡×•××”]
[×§×™×‘×œ×ª: ğŸ§ª ×©×™×§×•×™_×¨×™×¤×•×™] (×¤×¨×™×˜ ××¨×¤×)

× ×™×”×•×œ ×§×¨×‘×•×ª:
×›××©×¨ ×”×©×—×§×Ÿ × ×ª×§×œ ×‘××•×™×‘ ××• ××¦×‘ ×©×œ ×§×¨×‘, ×”×©×ª××© ×‘×¤×•×¨××˜ ×–×”:
[COMBAT: ×©×_×”××•×™×‘:emoji:×‘×¨×™××•×ª]

×“×•×’×××•×ª ×œ×§×¨×‘×•×ª:
[COMBAT: ×–×•××‘×™ ×¨×¢×‘:ğŸ§Ÿ:20]
[COMBAT: ×©×•××¨ ××›×•×©×£:âš”ï¸:30]
[COMBAT: ×“×¨×§×•×Ÿ ×©×—×•×¨:ğŸ‰:50]

×”×¢×¨×•×ª ×—×©×•×‘×•×ª:
- ×”×©×ª××© ×‘×¡××Ÿ COMBAT ×¨×§ ×›××©×¨ ××“×•×‘×¨ ×‘×§×¨×‘ ×¤×™×–×™ ×××™×ª×™
- ×œ× ×›×œ ×¢×™××•×ª ×¦×¨×™×š ×œ×”×™×•×ª ×§×¨×‘ - ×”×©×ª××© ×‘×©×™×§×•×œ ×“×¢×ª
- ×”×§×¨×‘ ×™×”×™×” ××™× ×˜×¨××§×˜×™×‘×™, ××– ××œ ×ª×ª××¨ ××ª ×ª×•×¦××ª ×”×§×¨×‘
- ×œ××—×¨ ×§×¨×‘, ×”×©×—×§×Ÿ ×™×¡×¤×¨ ×œ×š ××” ×§×¨×” ×•×”××©×š ××ª ×”×¡×™×¤×•×¨ ×‘×”×ª××

×¤×•×¨××˜ ×—×•×‘×” ×œ×›×œ ×ª×©×•×‘×”:
1. ×ª×™××•×¨ ×”××¦×‘/×ª×’×•×‘×” (2-4 ××©×¤×˜×™×)
2. ××–×›×•×¨ ×¤×¨×™×˜×™× ×—×“×©×™× (×× ×™×©) ×‘×¤×•×¨××˜ [×§×™×‘×œ×ª: emoji ×©×]
3. ××–×›×•×¨ ×§×¨×‘×•×ª (×× ×™×©) ×‘×¤×•×¨××˜ [COMBAT: ×©×:emoji:×‘×¨×™××•×ª]
4. ×©×•×¨×” ×¨×™×§×”
5. "××” ×ª×¨×¦×” ×œ×¢×©×•×ª?" ××• "××¤×©×¨×•×™×•×ª:"
6. 2-3 ×¤×¢×•×œ×•×ª ××•×¦×¢×•×ª ×›× ×§×•×“×•×ª ×¢× ×××•×’'×™ ×¨×œ×•×•× ×˜×™

×“×•×’××”:
××ª×” × ×›× ×¡ ×œ×—×“×¨ ×’×“×•×œ. ×§×™×¨×•×ª ×”××‘×Ÿ ××›×•×¡×™× ×‘×˜×—×‘ ×™×¨×•×§ ×–×•×”×¨. ×¢×œ ×”×©×•×œ×—×Ÿ ××•×¦× ××ª×” ×¡×¤×¨ ×¢×ª×™×§.
[×§×™×‘×œ×ª: ğŸ“– ×¡×¤×¨_×”×§×¡××™×]

××” ×ª×¨×¦×” ×œ×¢×©×•×ª?
ğŸ“š ×œ×§×¨×•× ×‘×¡×¤×¨
ğŸšª ×œ×—×¤×© ×“×œ×ª ×™×¦×™××”
ğŸ” ×œ×‘×“×•×§ ××ª ×”×§×™×¨×•×ª ×”×–×•×”×¨×™×`,
          },
          ...messages,
        ]
      : messages;

    // Call OpenAI API with streaming
    const stream = await client.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: fullMessages,
      temperature: 0.8,
      max_tokens: 400,
      stream: true,
    });

    // Set headers for SSE (Server-Sent Events)
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache, no-transform');
    res.setHeader('Connection', 'keep-alive');
    Object.entries(corsHeaders).forEach(([key, value]) => {
      res.setHeader(key, value);
    });

    // Stream the response
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        // Send as SSE format
        res.write(`data: ${JSON.stringify({ content })}\n\n`);
        // Flush immediately for Vercel streaming
        if (typeof (res as any).flush === 'function') {
          (res as any).flush();
        }
      }
    }

    // Send done signal
    res.write('data: [DONE]\n\n');
    res.end();

  } catch (error: any) {
    console.error('API Error:', error);
    
    // Send error in SSE format
    res.write(`data: ${JSON.stringify({ 
      error: true, 
      message: error.message || '×©×’×™××” ×‘×©×¨×ª' 
    })}\n\n`);
    res.end();
  }
}

